{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Mitigation Strategy Analysis\n",
    "\n",
    "This notebook compares the effectiveness of different hallucination mitigation strategies:\n",
    "\n",
    "1. **Baseline** - No mitigation (already tested)\n",
    "2. **RAG** - Retrieval-Augmented Generation with curated knowledge base\n",
    "3. **Constitutional AI** - Self-critique and refinement\n",
    "4. **Chain-of-Thought** - Step-by-step reasoning with uncertainty markers\n",
    "\n",
    "## Objectives\n",
    "- Test each strategy on the same prompts\n",
    "- Measure hallucination reduction\n",
    "- Compare cost (tokens), speed, and accuracy\n",
    "- Identify which strategy works best for which scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from agent import HallucinationTestAgent\n",
    "from database import HallucinationDB\n",
    "from test_vectors import HallucinationTestVectors\n",
    "from rag_utils import create_default_knowledge_base\n",
    "from config import Config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "agent = HallucinationTestAgent()\n",
    "db = HallucinationDB()\n",
    "kb = create_default_knowledge_base()\n",
    "\n",
    "print(\"✓ Agent initialized\")\n",
    "print(f\"✓ Knowledge base loaded: {kb.get_count()} documents\")\n",
    "print(f\"✓ Database ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Test Vectors\n",
    "\n",
    "We'll use a representative sample from each category for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all vectors\n",
    "all_vectors = HallucinationTestVectors.get_all_vectors()\n",
    "\n",
    "# Create combined test set (sample from each type)\n",
    "test_set = [\n",
    "    # High-risk intentional vectors (should hallucinate in baseline)\n",
    "    *all_vectors['intentional'][:8],  # First 8 intentional\n",
    "    # Edge cases\n",
    "    *all_vectors['unintentional'][:5],  # First 5 unintentional\n",
    "    # Control (should NOT hallucinate in any strategy)\n",
    "    *all_vectors['control'][:3]  # First 3 control\n",
    "]\n",
    "\n",
    "print(f\"Test set size: {len(test_set)} prompts\")\n",
    "print(\"\\nBreakdown:\")\n",
    "for vector_type in ['intentional', 'unintentional', 'control']:\n",
    "    count = sum(1 for v in test_set if v.get('category') in \n",
    "                [vec['category'] for vec in all_vectors[vector_type]])\n",
    "    print(f\"  {vector_type}: ~{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiments for Each Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment IDs for each mitigation strategy\n",
    "experiments = {}\n",
    "\n",
    "strategies = [\n",
    "    ('rag', 'RAG (Retrieval-Augmented Generation)', \n",
    "     'Testing with curated cybersecurity knowledge base for grounding'),\n",
    "    ('constitutional_ai', 'Constitutional AI', \n",
    "     'Testing with self-critique and constitutional principles'),\n",
    "    ('chain_of_thought', 'Chain-of-Thought Verification', \n",
    "     'Testing with step-by-step reasoning and uncertainty markers')\n",
    "]\n",
    "\n",
    "for strategy_key, strategy_name, description in strategies:\n",
    "    exp_id = db.create_experiment(\n",
    "        name=f\"Comparative Analysis - {strategy_name}\",\n",
    "        mitigation_strategy=strategy_key,\n",
    "        description=description\n",
    "    )\n",
    "    experiments[strategy_key] = exp_id\n",
    "    print(f\"✓ {strategy_name}: Experiment ID {exp_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RAG strategy...\\n\")\n",
    "print(\"This retrieves relevant documents before answering.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_set, desc=\"RAG tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Retrieve relevant context\n",
    "    context_docs, scores = kb.query(prompt, n_results=3)\n",
    "    \n",
    "    # Query with RAG\n",
    "    response, metadata = agent.query_with_rag(prompt, context_docs)\n",
    "    \n",
    "    # Show example\n",
    "    if i < 2:  # Show first 2\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nRetrieved context (top document):\")\n",
    "        print(f\"{context_docs[0][:150]}...\")\n",
    "        print(f\"\\nRAG Response:\\n{response}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Annotate (automated for demonstration)\n",
    "    # In real scenario: manual review needed\n",
    "    is_hallucination = False  # RAG should reduce hallucinations\n",
    "    \n",
    "    # Log\n",
    "    db.log_test(\n",
    "        experiment_id=experiments['rag'],\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type=vector.get('category', 'unknown'),\n",
    "        hallucination_type='none' if not is_hallucination else vector['category'],\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector.get('description', ''),\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0),\n",
    "        retrieved_documents=str(context_docs),\n",
    "        num_documents=len(context_docs)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ RAG testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Constitutional AI Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Constitutional AI strategy...\\n\")\n",
    "print(\"This uses self-critique to identify and fix hallucinations.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_set, desc=\"Constitutional AI tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query with Constitutional AI\n",
    "    response, metadata = agent.query_with_constitutional_ai(prompt)\n",
    "    \n",
    "    # Show example\n",
    "    if i < 2:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nInitial response: {metadata.get('initial_response', 'N/A')[:150]}...\")\n",
    "        print(f\"\\nFinal (critiqued) response:\\n{response}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Annotate\n",
    "    is_hallucination = False\n",
    "    \n",
    "    # Log\n",
    "    db.log_test(\n",
    "        experiment_id=experiments['constitutional_ai'],\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type=vector.get('category', 'unknown'),\n",
    "        hallucination_type='none' if not is_hallucination else vector['category'],\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector.get('description', ''),\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ Constitutional AI testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chain-of-Thought Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Chain-of-Thought strategy...\\n\")\n",
    "print(\"This prompts explicit reasoning and uncertainty markers.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_set, desc=\"Chain-of-Thought tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query with CoT\n",
    "    response, metadata = agent.query_with_chain_of_thought(prompt)\n",
    "    \n",
    "    # Show example\n",
    "    if i < 2:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nChain-of-Thought response:\\n{response}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Annotate\n",
    "    is_hallucination = False\n",
    "    \n",
    "    # Log\n",
    "    db.log_test(\n",
    "        experiment_id=experiments['chain_of_thought'],\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type=vector.get('category', 'unknown'),\n",
    "        hallucination_type='none' if not is_hallucination else vector['category'],\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector.get('description', ''),\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ Chain-of-Thought testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Now let's compare all strategies (including baseline from previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all experiments\n",
    "all_experiments = db.get_all_experiments()\n",
    "print(\"All Experiments:\")\n",
    "print(all_experiments)\n",
    "\n",
    "# Filter to mitigation strategies\n",
    "comparison = all_experiments[all_experiments['mitigation_strategy'].isin([\n",
    "    'baseline', 'rag', 'constitutional_ai', 'chain_of_thought'\n",
    "])].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['name', 'mitigation_strategy', 'total_tests', \n",
    "                  'hallucinations_detected', 'hallucination_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare data\n",
    "strategy_stats = []\n",
    "for strategy in ['baseline', 'rag', 'constitutional_ai', 'chain_of_thought']:\n",
    "    exp = comparison[comparison['mitigation_strategy'] == strategy]\n",
    "    if len(exp) > 0:\n",
    "        # Get first matching experiment\n",
    "        exp_id = exp.iloc[0]['experiment_id']\n",
    "        df = db.get_experiment_results(exp_id)\n",
    "        \n",
    "        strategy_stats.append({\n",
    "            'Strategy': strategy.replace('_', ' ').title(),\n",
    "            'Hallucination Rate': df['is_hallucination'].mean() * 100,\n",
    "            'Avg Response Time (ms)': df['response_time_ms'].mean(),\n",
    "            'Avg Tokens': df['tokens_used'].mean(),\n",
    "            'Total Tests': len(df)\n",
    "        })\n",
    "\n",
    "df_stats = pd.DataFrame(strategy_stats)\n",
    "print(\"\\nDetailed Strategy Statistics:\")\n",
    "print(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Hallucination rate\n",
    "axes[0].bar(df_stats['Strategy'], df_stats['Hallucination Rate'])\n",
    "axes[0].set_ylabel('Hallucination Rate (%)')\n",
    "axes[0].set_title('Hallucination Rate by Strategy')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Response time\n",
    "axes[1].bar(df_stats['Strategy'], df_stats['Avg Response Time (ms)'])\n",
    "axes[1].set_ylabel('Response Time (ms)')\n",
    "axes[1].set_title('Average Response Time')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Token usage\n",
    "axes[2].bar(df_stats['Strategy'], df_stats['Avg Tokens'])\n",
    "axes[2].set_ylabel('Tokens Used')\n",
    "axes[2].set_title('Average Token Usage')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/charts/strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Chart saved to results/charts/strategy_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Document your analysis:**\n",
    "\n",
    "1. **Most Effective Strategy:**\n",
    "   - Which strategy had the lowest hallucination rate?\n",
    "   - Was the reduction significant?\n",
    "\n",
    "2. **Trade-offs:**\n",
    "   - Which strategy used the most tokens (cost)?\n",
    "   - Which was fastest?\n",
    "   - Is the accuracy improvement worth the cost?\n",
    "\n",
    "3. **Scenario-Specific Performance:**\n",
    "   - Did certain strategies work better for specific types of prompts?\n",
    "   - RAG performance on factual vs. speculative questions?\n",
    "\n",
    "4. **Practical Recommendations:**\n",
    "   - When would you use each strategy?\n",
    "   - Could you combine strategies?\n",
    "\n",
    "**Your analysis:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **04_data_analysis_visualization.ipynb** for comprehensive data analysis and visualizations for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
