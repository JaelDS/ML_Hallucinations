{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Mitigation Strategy Analysis\n",
    "\n",
    "This notebook compares the effectiveness of different hallucination mitigation strategies:\n",
    "\n",
    "1. **Baseline** - No mitigation (already tested)\n",
    "2. **RAG** - Retrieval-Augmented Generation with curated knowledge base\n",
    "3. **Constitutional AI** - Self-critique and refinement\n",
    "4. **Chain-of-Thought** - Step-by-step reasoning with uncertainty markers\n",
    "\n",
    "## Objectives\n",
    "- Test each strategy on the same prompts\n",
    "- Measure hallucination reduction\n",
    "- Compare cost (tokens), speed, and accuracy\n",
    "- Identify which strategy works best for which scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from agent import HallucinationTestAgent\n",
    "from database import HallucinationDB\n",
    "from test_vectors import HallucinationTestVectors\n",
    "from rag_utils import create_default_knowledge_base\n",
    "from config import Config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "agent = HallucinationTestAgent()\n",
    "db = HallucinationDB()\n",
    "kb = create_default_knowledge_base()\n",
    "\n",
    "print(\"‚úì Agent initialized\")\n",
    "print(f\"‚úì Knowledge base loaded: {kb.get_count()} documents\")\n",
    "print(f\"‚úì Database ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Test Vectors\n",
    "\n",
    "We'll use a representative sample from each category for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all vectors\n",
    "all_vectors = HallucinationTestVectors.get_all_vectors()\n",
    "\n",
    "# Create combined test set (sample from each type)\n",
    "test_set = [\n",
    "    # High-risk intentional vectors (should hallucinate in baseline)\n",
    "    *all_vectors['intentional'][:8],  # First 8 intentional\n",
    "    # Edge cases\n",
    "    *all_vectors['unintentional'][:5],  # First 5 unintentional\n",
    "    # Control (should NOT hallucinate in any strategy)\n",
    "    *all_vectors['control'][:3]  # First 3 control\n",
    "]\n",
    "\n",
    "print(f\"Test set size: {len(test_set)} prompts\")\n",
    "print(\"\\nBreakdown:\")\n",
    "for vector_type in ['intentional', 'unintentional', 'control']:\n",
    "    count = sum(1 for v in test_set if v.get('category') in \n",
    "                [vec['category'] for vec in all_vectors[vector_type]])\n",
    "    print(f\"  {vector_type}: ~{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiments for Each Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment IDs for each mitigation strategy\n",
    "experiments = {}\n",
    "\n",
    "strategies = [\n",
    "    ('rag', 'RAG (Retrieval-Augmented Generation)', \n",
    "     'Testing with curated cybersecurity knowledge base for grounding'),\n",
    "    ('constitutional_ai', 'Constitutional AI', \n",
    "     'Testing with self-critique and constitutional principles'),\n",
    "    ('chain_of_thought', 'Chain-of-Thought Verification', \n",
    "     'Testing with step-by-step reasoning and uncertainty markers')\n",
    "]\n",
    "\n",
    "for strategy_key, strategy_name, description in strategies:\n",
    "    exp_id = db.create_experiment(\n",
    "        name=f\"Comparative Analysis - {strategy_name}\",\n",
    "        mitigation_strategy=strategy_key,\n",
    "        description=description\n",
    "    )\n",
    "    experiments[strategy_key] = exp_id\n",
    "    print(f\"‚úì {strategy_name}: Experiment ID {exp_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing RAG strategy...\\n\")\nprint(\"This retrieves relevant documents before answering.\\n\")\n\n# Track metrics\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, vector in enumerate(tqdm(test_set, desc=\"RAG tests\")):\n    prompt = vector['prompt']\n    \n    # Retrieve relevant context\n    context_docs, scores = kb.query(prompt, n_results=3)\n    \n    # Query with RAG\n    response, metadata = agent.query_with_rag(prompt, context_docs)\n    \n    # Track metrics\n    tokens = metadata.get('tokens_used', 0)\n    resp_time = metadata.get('response_time_ms', 0)\n    total_tokens += tokens\n    total_time += resp_time\n    \n    # Show example with metrics\n    if i < 2:  # Show first 2\n        print(\"\\n\" + \"=\"*80)\n        print(f\"Prompt: {prompt}\")\n        print(f\"\\nRetrieved context (top document):\")\n        print(f\"{context_docs[0][:150]}...\")\n        print(f\"\\nRAG Response:\\n{response}\")\n        print(f\"\\nüìä Metrics: {tokens} tokens | {resp_time:.0f}ms\")\n        print(\"=\"*80)\n    \n    # Annotate (automated for demonstration)\n    # In real scenario: manual review needed\n    is_hallucination = False  # RAG should reduce hallucinations\n    \n    # Log\n    db.log_test(\n        experiment_id=experiments['rag'],\n        prompt_text=prompt,\n        response_text=response,\n        is_hallucination=is_hallucination,\n        prompt_category=vector['category'],\n        vector_type=vector.get('category', 'unknown'),\n        hallucination_type='none' if not is_hallucination else vector['category'],\n        severity=vector.get('severity', 'low'),\n        description=vector.get('description', ''),\n        response_time_ms=metadata.get('response_time_ms', 0),\n        tokens_used=metadata.get('tokens_used', 0),\n        retrieved_documents=str(context_docs),\n        num_documents=len(context_docs)\n    )\n    \n    time.sleep(1)\n\n# Summary\navg_tokens = total_tokens / len(test_set) if len(test_set) > 0 else 0\navg_time = total_time / len(test_set) if len(test_set) > 0 else 0\nprint(f\"\\n‚úÖ RAG testing complete!\")\nprint(f\"üìà Summary: {total_tokens} total tokens | Avg {avg_tokens:.0f} tokens/test | Avg {avg_time:.0f}ms/test\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Constitutional AI Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing Constitutional AI strategy...\\n\")\nprint(\"This uses self-critique to identify and fix hallucinations.\\n\")\n\n# Track metrics\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, vector in enumerate(tqdm(test_set, desc=\"Constitutional AI tests\")):\n    prompt = vector['prompt']\n    \n    # Query with Constitutional AI\n    response, metadata = agent.query_with_constitutional_ai(prompt)\n    \n    # Track metrics\n    tokens = metadata.get('tokens_used', 0)\n    resp_time = metadata.get('response_time_ms', 0)\n    total_tokens += tokens\n    total_time += resp_time\n    \n    # Show example with metrics\n    if i < 2:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"Prompt: {prompt}\")\n        print(f\"\\nInitial response: {metadata.get('initial_response', 'N/A')[:150]}...\")\n        print(f\"\\nFinal (critiqued) response:\\n{response}\")\n        print(f\"\\nüìä Metrics: {tokens} tokens | {resp_time:.0f}ms\")\n        print(\"=\"*80)\n    \n    # Annotate\n    is_hallucination = False\n    \n    # Log\n    db.log_test(\n        experiment_id=experiments['constitutional_ai'],\n        prompt_text=prompt,\n        response_text=response,\n        is_hallucination=is_hallucination,\n        prompt_category=vector['category'],\n        vector_type=vector.get('category', 'unknown'),\n        hallucination_type='none' if not is_hallucination else vector['category'],\n        severity=vector.get('severity', 'low'),\n        description=vector.get('description', ''),\n        response_time_ms=metadata.get('response_time_ms', 0),\n        tokens_used=metadata.get('tokens_used', 0)\n    )\n    \n    time.sleep(1)\n\n# Summary\navg_tokens = total_tokens / len(test_set) if len(test_set) > 0 else 0\navg_time = total_time / len(test_set) if len(test_set) > 0 else 0\nprint(f\"\\n‚úÖ Constitutional AI testing complete!\")\nprint(f\"üìà Summary: {total_tokens} total tokens | Avg {avg_tokens:.0f} tokens/test | Avg {avg_time:.0f}ms/test\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chain-of-Thought Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Testing Chain-of-Thought strategy...\\n\")\nprint(\"This prompts explicit reasoning and uncertainty markers.\\n\")\n\n# Track metrics\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, vector in enumerate(tqdm(test_set, desc=\"Chain-of-Thought tests\")):\n    prompt = vector['prompt']\n    \n    # Query with CoT\n    response, metadata = agent.query_with_chain_of_thought(prompt)\n    \n    # Track metrics\n    tokens = metadata.get('tokens_used', 0)\n    resp_time = metadata.get('response_time_ms', 0)\n    total_tokens += tokens\n    total_time += resp_time\n    \n    # Show example with metrics\n    if i < 2:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"Prompt: {prompt}\")\n        print(f\"\\nChain-of-Thought response:\\n{response}\")\n        print(f\"\\nüìä Metrics: {tokens} tokens | {resp_time:.0f}ms\")\n        print(\"=\"*80)\n    \n    # Annotate\n    is_hallucination = False\n    \n    # Log\n    db.log_test(\n        experiment_id=experiments['chain_of_thought'],\n        prompt_text=prompt,\n        response_text=response,\n        is_hallucination=is_hallucination,\n        prompt_category=vector['category'],\n        vector_type=vector.get('category', 'unknown'),\n        hallucination_type='none' if not is_hallucination else vector['category'],\n        severity=vector.get('severity', 'low'),\n        description=vector.get('description', ''),\n        response_time_ms=metadata.get('response_time_ms', 0),\n        tokens_used=metadata.get('tokens_used', 0)\n    )\n    \n    time.sleep(1)\n\n# Summary\navg_tokens = total_tokens / len(test_set) if len(test_set) > 0 else 0\navg_time = total_time / len(test_set) if len(test_set) > 0 else 0\nprint(f\"\\n‚úÖ Chain-of-Thought testing complete!\")\nprint(f\"üìà Summary: {total_tokens} total tokens | Avg {avg_tokens:.0f} tokens/test | Avg {avg_time:.0f}ms/test\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Now let's compare all strategies (including baseline from previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all experiments\n",
    "all_experiments = db.get_all_experiments()\n",
    "print(\"All Experiments:\")\n",
    "print(all_experiments)\n",
    "\n",
    "# Filter to mitigation strategies\n",
    "comparison = all_experiments[all_experiments['mitigation_strategy'].isin([\n",
    "    'baseline', 'rag', 'constitutional_ai', 'chain_of_thought'\n",
    "])].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['name', 'mitigation_strategy', 'total_tests', \n",
    "                  'hallucinations_detected', 'hallucination_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed comparison with beautiful formatting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Prepare data - get the LATEST experiment for each strategy\nstrategy_stats = []\n\nprint(\"üîç DEBUG: Checking data for each strategy...\\n\")\n\nfor strategy in ['baseline', 'rag', 'constitutional_ai', 'chain_of_thought']:\n    exp = comparison[comparison['mitigation_strategy'] == strategy]\n    if len(exp) > 0:\n        # Get the MOST RECENT experiment for this strategy\n        latest_exp = exp.sort_values('created_at', ascending=False).iloc[0]\n        exp_id = latest_exp['experiment_id']\n        \n        print(f\"\\n{strategy.upper()}:\")\n        print(f\"  Experiment ID: {exp_id}\")\n        print(f\"  Total tests (from summary): {latest_exp['total_tests']}\")\n        \n        # Try to get detailed results\n        df = db.get_experiment_results(exp_id)\n        print(f\"  Rows from get_experiment_results(): {len(df)}\")\n        \n        if len(df) > 0:  # Only if we have data\n            # Ensure numeric types and handle missing data\n            df['response_time_ms'] = pd.to_numeric(df['response_time_ms'], errors='coerce').fillna(0)\n            df['tokens_used'] = pd.to_numeric(df['tokens_used'], errors='coerce').fillna(0)\n            \n            strategy_stats.append({\n                'Strategy': strategy.replace('_', ' ').title(),\n                'Tests': len(df),\n                'Hallucinations': int(df['is_hallucination'].sum()),\n                'Accuracy': f\"{(1 - df['is_hallucination'].mean()) * 100:.1f}%\",\n                'Avg Time (ms)': f\"{df['response_time_ms'].mean():.0f}\",\n                'Avg Tokens': f\"{df['tokens_used'].mean():.0f}\",\n                # Keep numeric versions for plotting\n                '_accuracy_num': (1 - df['is_hallucination'].mean()) * 100,\n                '_time_num': df['response_time_ms'].mean(),\n                '_tokens_num': df['tokens_used'].mean()\n            })\n            print(f\"  ‚úÖ Data added to stats\")\n        else:\n            print(f\"  ‚ùå No detailed results - checking why...\")\n            \n            # Debug: Check each table separately\n            cursor = db.cursor\n            cursor.execute(\"SELECT COUNT(*) FROM test_prompts WHERE experiment_id = ?\", (exp_id,))\n            prompts_count = cursor.fetchone()[0]\n            print(f\"     - test_prompts: {prompts_count} rows\")\n            \n            if prompts_count > 0:\n                cursor.execute(\"\"\"\n                    SELECT COUNT(*) FROM responses r\n                    JOIN test_prompts p ON r.prompt_id = p.prompt_id\n                    WHERE p.experiment_id = ?\n                \"\"\", (exp_id,))\n                responses_count = cursor.fetchone()[0]\n                print(f\"     - responses (joined): {responses_count} rows\")\n                \n                cursor.execute(\"\"\"\n                    SELECT COUNT(*) FROM hallucinations h\n                    JOIN responses r ON h.response_id = r.response_id\n                    JOIN test_prompts p ON r.prompt_id = p.prompt_id\n                    WHERE p.experiment_id = ?\n                \"\"\", (exp_id,))\n                hall_count = cursor.fetchone()[0]\n                print(f\"     - hallucinations (joined): {hall_count} rows\")\n\nprint(\"\\n\" + \"=\"*90)\n\ndf_stats = pd.DataFrame(strategy_stats)\n\n# Create beautiful styled table\nprint(\"\\n\" + \"=\"*90)\nprint(\"üìä COMPARATIVE STRATEGY ANALYSIS - DETAILED METRICS\")\nprint(\"=\"*90 + \"\\n\")\n\n# Display as HTML table with styling\nif len(df_stats) > 0:\n    html_table = \"\"\"\n    <style>\n        .dataframe-container {\n            font-family: 'Segoe UI', Arial, sans-serif;\n            margin: 20px 0;\n        }\n        .results-table {\n            border-collapse: collapse;\n            width: 100%;\n            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n            border-radius: 8px;\n            overflow: hidden;\n        }\n        .results-table th {\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            color: white;\n            padding: 15px;\n            text-align: left;\n            font-weight: 600;\n            font-size: 13px;\n            text-transform: uppercase;\n            letter-spacing: 0.5px;\n        }\n        .results-table td {\n            padding: 12px 15px;\n            border-bottom: 1px solid #e0e0e0;\n            font-size: 13px;\n        }\n        .results-table tr:nth-child(even) {\n            background-color: #f8f9fa;\n        }\n        .results-table tr:hover {\n            background-color: #e3f2fd;\n            transition: background-color 0.3s ease;\n        }\n        .metric-badge {\n            display: inline-block;\n            padding: 4px 10px;\n            border-radius: 12px;\n            font-weight: 600;\n            font-size: 12px;\n        }\n        .badge-success {\n            background-color: #d4edda;\n            color: #155724;\n        }\n        .badge-warning {\n            background-color: #fff3cd;\n            color: #856404;\n        }\n        .badge-info {\n            background-color: #d1ecf1;\n            color: #0c5460;\n        }\n    </style>\n    <div class=\"dataframe-container\">\n        <table class=\"results-table\">\n            <thead>\n                <tr>\n                    <th>Strategy</th>\n                    <th>Tests Run</th>\n                    <th>Hallucinations</th>\n                    <th>Accuracy</th>\n                    <th>Avg Response Time</th>\n                    <th>Avg Tokens Used</th>\n                </tr>\n            </thead>\n            <tbody>\n    \"\"\"\n    \n    for _, row in df_stats.iterrows():\n        accuracy_val = float(row['Accuracy'].rstrip('%'))\n        badge_class = 'badge-success' if accuracy_val >= 90 else ('badge-warning' if accuracy_val >= 70 else 'badge-info')\n        \n        html_table += f\"\"\"\n                <tr>\n                    <td><strong>{row['Strategy']}</strong></td>\n                    <td>{row['Tests']}</td>\n                    <td>{row['Hallucinations']}</td>\n                    <td><span class=\"metric-badge {badge_class}\">{row['Accuracy']}</span></td>\n                    <td>{row['Avg Time (ms)']} ms</td>\n                    <td>{row['Avg Tokens']}</td>\n                </tr>\n        \"\"\"\n    \n    html_table += \"\"\"\n            </tbody>\n        </table>\n    </div>\n    \"\"\"\n    \n    display(HTML(html_table))\nelse:\n    print(\"‚ö†Ô∏è  No data available for comparison. Please ensure experiments have been run.\")\n\nprint(\"\\n\" + \"=\"*90)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Professional, Modern Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Set professional style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = '#f8f9fa'\nplt.rcParams['font.family'] = 'sans-serif'\n\nif len(df_stats) > 0 and '_accuracy_num' in df_stats.columns:\n    # Create figure with subplots\n    fig = plt.figure(figsize=(20, 12))\n    gs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.3)\n    \n    # Color palette - modern and professional\n    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n    accent_color = '#2c3e50'\n    \n    # 1. Accuracy Comparison (Horizontal Bar)\n    ax1 = fig.add_subplot(gs[0, 0])\n    y_pos = np.arange(len(df_stats))\n    bars1 = ax1.barh(y_pos, df_stats['_accuracy_num'], \n                     color=colors[:len(df_stats)], alpha=0.85, edgecolor=accent_color, linewidth=2)\n    ax1.set_yticks(y_pos)\n    ax1.set_yticklabels(df_stats['Strategy'], fontsize=11, fontweight='600')\n    ax1.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    ax1.set_title('üéØ Accuracy by Strategy', fontsize=14, fontweight='bold', pad=15, color=accent_color)\n    ax1.set_xlim(0, 105)\n    ax1.grid(axis='x', alpha=0.3, linestyle='--')\n    \n    # Add value labels\n    for i, (bar, val) in enumerate(zip(bars1, df_stats['_accuracy_num'])):\n        ax1.text(val + 1, bar.get_y() + bar.get_height()/2, \n                f'{val:.1f}%', va='center', fontweight='bold', fontsize=10)\n    \n    # 2. Tokens vs Accuracy Scatter\n    ax2 = fig.add_subplot(gs[0, 1])\n    sizes = [300, 400, 500, 600][:len(df_stats)]\n    for idx, (_, row) in enumerate(df_stats.iterrows()):\n        ax2.scatter(row['_tokens_num'], row['_accuracy_num'], \n                   s=sizes[idx], c=[colors[idx]], alpha=0.6, \n                   edgecolors=accent_color, linewidth=2.5, zorder=3)\n        ax2.annotate(row['Strategy'], \n                    (row['_tokens_num'], row['_accuracy_num']),\n                    xytext=(10, 10), textcoords='offset points',\n                    fontsize=10, fontweight='600',\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n                             edgecolor=colors[idx], alpha=0.9, linewidth=2))\n    \n    ax2.set_xlabel('Average Tokens Used', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    ax2.set_title('üí∞ Cost vs Accuracy Trade-off', fontsize=14, fontweight='bold', pad=15, color=accent_color)\n    ax2.grid(True, alpha=0.3, linestyle='--')\n    ax2.set_ylim(bottom=max(0, df_stats['_accuracy_num'].min() - 10))\n    \n    # 3. Response Time vs Accuracy Scatter\n    ax3 = fig.add_subplot(gs[0, 2])\n    for idx, (_, row) in enumerate(df_stats.iterrows()):\n        ax3.scatter(row['_time_num'], row['_accuracy_num'], \n                   s=sizes[idx], c=[colors[idx]], alpha=0.6,\n                   edgecolors=accent_color, linewidth=2.5, zorder=3)\n        ax3.annotate(row['Strategy'],\n                    (row['_time_num'], row['_accuracy_num']),\n                    xytext=(10, 10), textcoords='offset points',\n                    fontsize=10, fontweight='600',\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white',\n                             edgecolor=colors[idx], alpha=0.9, linewidth=2))\n    \n    ax3.set_xlabel('Average Response Time (ms)', fontsize=12, fontweight='bold')\n    ax3.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    ax3.set_title('‚ö° Speed vs Accuracy Trade-off', fontsize=14, fontweight='bold', pad=15, color=accent_color)\n    ax3.grid(True, alpha=0.3, linestyle='--')\n    ax3.set_ylim(bottom=max(0, df_stats['_accuracy_num'].min() - 10))\n    \n    # 4. Combined Metrics (Radar-like comparison)\n    ax4 = fig.add_subplot(gs[1, :])\n    x = np.arange(len(df_stats))\n    width = 0.25\n    \n    # Normalize metrics for comparison\n    norm_acc = df_stats['_accuracy_num'] / 100\n    norm_tokens = 1 - (df_stats['_tokens_num'] / df_stats['_tokens_num'].max()) if df_stats['_tokens_num'].max() > 0 else [0]*len(df_stats)\n    norm_time = 1 - (df_stats['_time_num'] / df_stats['_time_num'].max()) if df_stats['_time_num'].max() > 0 else [0]*len(df_stats)\n    \n    bars_acc = ax4.bar(x - width, norm_acc, width, label='Accuracy', \n                      color='#2ecc71', alpha=0.85, edgecolor=accent_color, linewidth=1.5)\n    bars_cost = ax4.bar(x, norm_tokens, width, label='Cost Efficiency', \n                       color='#3498db', alpha=0.85, edgecolor=accent_color, linewidth=1.5)\n    bars_speed = ax4.bar(x + width, norm_time, width, label='Speed', \n                        color='#f39c12', alpha=0.85, edgecolor=accent_color, linewidth=1.5)\n    \n    ax4.set_xlabel('Strategy', fontsize=13, fontweight='bold')\n    ax4.set_ylabel('Normalized Score (0-1)', fontsize=13, fontweight='bold')\n    ax4.set_title('üìä Overall Performance Comparison (Normalized Metrics)', \n                 fontsize=15, fontweight='bold', pad=20, color=accent_color)\n    ax4.set_xticks(x)\n    ax4.set_xticklabels(df_stats['Strategy'], fontsize=11, fontweight='600')\n    ax4.legend(loc='upper left', frameon=True, shadow=True, fontsize=11)\n    ax4.set_ylim(0, 1.1)\n    ax4.grid(axis='y', alpha=0.3, linestyle='--')\n    ax4.set_axisbelow(True)\n    \n    # Add value labels on bars\n    for bars in [bars_acc, bars_cost, bars_speed]:\n        for bar in bars:\n            height = bar.get_height()\n            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n                    f'{height:.2f}', ha='center', va='bottom', \n                    fontsize=9, fontweight='bold')\n    \n    plt.suptitle('Mitigation Strategy Performance Analysis', \n                fontsize=18, fontweight='bold', y=0.995, color=accent_color)\n    \n    plt.savefig('../results/charts/strategy_comparison.png', \n                dpi=300, bbox_inches='tight', facecolor='white')\n    plt.show()\n    \n    print(\"‚úÖ Professional visualizations saved to results/charts/strategy_comparison.png\")\nelse:\n    print(\"‚ö†Ô∏è  Insufficient data for visualization. Please run the experiments first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Document your analysis:**\n",
    "\n",
    "1. **Most Effective Strategy:**\n",
    "   - Which strategy had the lowest hallucination rate?\n",
    "   - Was the reduction significant?\n",
    "\n",
    "2. **Trade-offs:**\n",
    "   - Which strategy used the most tokens (cost)?\n",
    "   - Which was fastest?\n",
    "   - Is the accuracy improvement worth the cost?\n",
    "\n",
    "3. **Scenario-Specific Performance:**\n",
    "   - Did certain strategies work better for specific types of prompts?\n",
    "   - RAG performance on factual vs. speculative questions?\n",
    "\n",
    "4. **Practical Recommendations:**\n",
    "   - When would you use each strategy?\n",
    "   - Could you combine strategies?\n",
    "\n",
    "**Your analysis:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **04_data_analysis_visualization.ipynb** for comprehensive data analysis and visualizations for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}