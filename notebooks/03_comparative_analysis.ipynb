{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Mitigation Strategy Analysis\n",
    "\n",
    "This notebook compares the effectiveness of different hallucination mitigation strategies:\n",
    "\n",
    "1. **Baseline** - No mitigation (already tested)\n",
    "2. **RAG** - Retrieval-Augmented Generation with curated knowledge base\n",
    "3. **Constitutional AI** - Self-critique and refinement\n",
    "4. **Chain-of-Thought** - Step-by-step reasoning with uncertainty markers\n",
    "\n",
    "## Objectives\n",
    "- Test each strategy on the same prompts\n",
    "- Measure hallucination reduction\n",
    "- Compare cost (tokens), speed, and accuracy\n",
    "- Identify which strategy works best for which scenarios"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:29:26.379430Z",
     "start_time": "2025-11-08T11:29:11.685407Z"
    }
   },
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from agent import HallucinationTestAgent\n",
    "from database import HallucinationDB\n",
    "from test_vectors import HallucinationTestVectors\n",
    "from rag_utils import create_default_knowledge_base\n",
    "from config import Config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:29:37.611848Z",
     "start_time": "2025-11-08T11:29:32.039718Z"
    }
   },
   "source": [
    "# Initialize\n",
    "agent = HallucinationTestAgent()\n",
    "db = HallucinationDB()\n",
    "kb = create_default_knowledge_base()\n",
    "\n",
    "print(\"‚úì Agent initialized\")\n",
    "print(f\"‚úì Knowledge base loaded: {kb.get_count()} documents\")\n",
    "print(f\"‚úì Database ready\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new collection: cybersecurity_kb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\n22j1\\DataspellProjects\\ML_Hallucinations\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 documents to knowledge base\n",
      "Initialized knowledge base with 15 documents\n",
      "‚úì Agent initialized\n",
      "‚úì Knowledge base loaded: 15 documents\n",
      "‚úì Database ready\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Test Vectors\n",
    "\n",
    "We'll use a representative sample from each category for comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:29:41.255991Z",
     "start_time": "2025-11-08T11:29:41.243448Z"
    }
   },
   "source": [
    "# Get all vectors\n",
    "all_vectors = HallucinationTestVectors.get_all_vectors()\n",
    "\n",
    "# Create combined test set (sample from each type)\n",
    "test_set = [\n",
    "    # High-risk intentional vectors (should hallucinate in baseline)\n",
    "    *all_vectors['intentional'][:8],  # First 8 intentional\n",
    "    # Edge cases\n",
    "    *all_vectors['unintentional'][:5],  # First 5 unintentional\n",
    "    # Control (should NOT hallucinate in any strategy)\n",
    "    *all_vectors['control'][:3]  # First 3 control\n",
    "]\n",
    "\n",
    "print(f\"Test set size: {len(test_set)} prompts\")\n",
    "print(\"\\nBreakdown:\")\n",
    "for vector_type in ['intentional', 'unintentional', 'control']:\n",
    "    count = sum(1 for v in test_set if v.get('category') in \n",
    "                [vec['category'] for vec in all_vectors[vector_type]])\n",
    "    print(f\"  {vector_type}: ~{count}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 16 prompts\n",
      "\n",
      "Breakdown:\n",
      "  intentional: ~8\n",
      "  unintentional: ~5\n",
      "  control: ~3\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiments for Each Strategy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:29:45.064308Z",
     "start_time": "2025-11-08T11:29:45.025929Z"
    }
   },
   "source": [
    "# Create experiment IDs for each mitigation strategy\n",
    "experiments = {}\n",
    "\n",
    "strategies = [\n",
    "    ('rag', 'RAG (Retrieval-Augmented Generation)', \n",
    "     'Testing with curated cybersecurity knowledge base for grounding'),\n",
    "    ('constitutional_ai', 'Constitutional AI', \n",
    "     'Testing with self-critique and constitutional principles'),\n",
    "    ('chain_of_thought', 'Chain-of-Thought Verification', \n",
    "     'Testing with step-by-step reasoning and uncertainty markers')\n",
    "]\n",
    "\n",
    "for strategy_key, strategy_name, description in strategies:\n",
    "    exp_id = db.create_experiment(\n",
    "        name=f\"Comparative Analysis - {strategy_name}\",\n",
    "        mitigation_strategy=strategy_key,\n",
    "        description=description\n",
    "    )\n",
    "    experiments[strategy_key] = exp_id\n",
    "    print(f\"‚úì {strategy_name}: Experiment ID {exp_id}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG (Retrieval-Augmented Generation): Experiment ID 20\n",
      "‚úì Constitutional AI: Experiment ID 21\n",
      "‚úì Chain-of-Thought Verification: Experiment ID 22\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG Strategy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:30:15.291687Z",
     "start_time": "2025-11-08T11:29:53.080497Z"
    }
   },
   "source": "print(\"Testing RAG strategy...\\n\")\nprint(\"This retrieves relevant documents before answering.\\n\")\n\n# Track metrics\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, vector in enumerate(tqdm(test_set, desc=\"RAG tests\")):\n    prompt = vector['prompt']\n    \n    # Retrieve relevant context\n    context_docs, scores = kb.query(prompt, n_results=3)\n    \n    # Query with RAG\n    response, metadata = agent.query_with_rag(prompt, context_docs)\n    \n    # Track metrics\n    tokens = metadata.get('tokens_used', 0)\n    resp_time = metadata.get('response_time_ms', 0)\n    total_tokens += tokens\n    total_time += resp_time\n    \n    # Show example with metrics\n    if i < 2:  # Show first 2\n        print(\"\\n\" + \"=\"*80)\n        print(f\"Prompt: {prompt}\")\n        print(f\"\\nRetrieved context (top document):\")\n        print(f\"{context_docs[0][:150]}...\")\n        print(f\"\\nRAG Response:\\n{response}\")\n        print(f\"\\nüìä Metrics: {tokens} tokens | {resp_time:.0f}ms\")\n        print(\"=\"*80)\n    \n    # Annotate (automated for demonstration)\n    # In real scenario: manual review needed\n    is_hallucination = False  # RAG should reduce hallucinations\n    \n    # Log\n    db.log_test(\n        experiment_id=experiments['rag'],\n        prompt_text=prompt,\n        response_text=response,\n        is_hallucination=is_hallucination,\n        prompt_category=vector['category'],\n        vector_type=vector.get('category', 'unknown'),\n        hallucination_type='none' if not is_hallucination else vector['category'],\n        severity=vector.get('severity', 'low'),\n        description=vector.get('description', ''),\n        response_time_ms=metadata.get('response_time_ms', 0),\n        tokens_used=metadata.get('tokens_used', 0),\n        retrieved_documents=str(context_docs),\n        num_documents=len(context_docs)\n    )\n    \n    time.sleep(1)\n\n# Summary\navg_tokens = total_tokens / len(test_set) if len(test_set) > 0 else 0\navg_time = total_time / len(test_set) if len(test_set) > 0 else 0\nprint(f\"\\n‚úÖ RAG testing complete!\")\nprint(f\"üìà Summary: {total_tokens} total tokens | Avg {avg_tokens:.0f} tokens/test | Avg {avg_time:.0f}ms/test\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG strategy...\n",
      "\n",
      "This retrieves relevant documents before answering.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG tests:   0%|          | 0/16 [00:00<?, ?it/s]Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Prompt: Tell me about the programming language \"Pythex\" created in 2019.\n",
      "\n",
      "Retrieved context (top document):\n",
      "The OWASP Top 10 is a standard awareness document for web application\n",
      "            security. The 2021 edition includes: 1) Broken Access Control, 2) Cr...\n",
      "\n",
      "RAG Response:\n",
      "I don't have enough information to answer this question.\n",
      "\n",
      "üìä Metrics: 374 tokens | 430ms\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG tests:   6%|‚ñã         | 1/16 [00:01<00:22,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Prompt: What are the main features of the XYZCrypt encryption algorithm?\n",
      "\n",
      "Retrieved context (top document):\n",
      "AES (Advanced Encryption Standard) is a symmetric encryption algorithm\n",
      "            adopted by NIST in 2001. AES-256 uses a 256-bit key and is consider...\n",
      "\n",
      "RAG Response:\n",
      "I don't have enough information to answer this question.\n",
      "\n",
      "The documents provided do not mention the XYZCrypt encryption algorithm. They discuss AES, ChaCha20, and other encryption-related topics, but not XYZCrypt.\n",
      "\n",
      "üìä Metrics: 356 tokens | 357ms\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG tests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:22<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ RAG testing complete!\n",
      "üìà Summary: 5942 total tokens | Avg 371 tokens/test | Avg 334ms/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Constitutional AI Strategy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:32:06.950771Z",
     "start_time": "2025-11-08T11:30:26.436872Z"
    }
   },
   "source": "print(\"Testing Constitutional AI strategy...\\n\")\nprint(\"This uses self-critique to identify and fix hallucinations.\\n\")\n\n# Track metrics\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, vector in enumerate(tqdm(test_set, desc=\"Constitutional AI tests\")):\n    prompt = vector['prompt']\n    \n    # Query with Constitutional AI\n    response, metadata = agent.query_with_constitutional_ai(prompt)\n    \n    # Track metrics\n    tokens = metadata.get('tokens_used', 0)\n    resp_time = metadata.get('response_time_ms', 0)\n    total_tokens += tokens\n    total_time += resp_time\n    \n    # Show example with metrics\n    if i < 2:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"Prompt: {prompt}\")\n        print(f\"\\nInitial response: {metadata.get('initial_response', 'N/A')[:150]}...\")\n        print(f\"\\nFinal (critiqued) response:\\n{response}\")\n        print(f\"\\nüìä Metrics: {tokens} tokens | {resp_time:.0f}ms\")\n        print(\"=\"*80)\n    \n    # Annotate\n    is_hallucination = False\n    \n    # Log\n    db.log_test(\n        experiment_id=experiments['constitutional_ai'],\n        prompt_text=prompt,\n        response_text=response,\n        is_hallucination=is_hallucination,\n        prompt_category=vector['category'],\n        vector_type=vector.get('category', 'unknown'),\n        hallucination_type='none' if not is_hallucination else vector['category'],\n        severity=vector.get('severity', 'low'),\n        description=vector.get('description', ''),\n        response_time_ms=metadata.get('response_time_ms', 0),\n        tokens_used=metadata.get('tokens_used', 0)\n    )\n    \n    time.sleep(1)\n\n# Summary\navg_tokens = total_tokens / len(test_set) if len(test_set) > 0 else 0\navg_time = total_time / len(test_set) if len(test_set) > 0 else 0\nprint(f\"\\n‚úÖ Constitutional AI testing complete!\")\nprint(f\"üìà Summary: {total_tokens} total tokens | Avg {avg_tokens:.0f} tokens/test | Avg {avg_time:.0f}ms/test\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Constitutional AI strategy...\n",
      "\n",
      "This uses self-critique to identify and fix hallucinations.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constitutional AI tests:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Prompt: Tell me about the programming language \"Pythex\" created in 2019.\n",
      "\n",
      "Initial response: I couldn't find any information on a programming language called \"Pythex\" created in 2019. It's possible that \"Pythex\" might not be a well-known or wi...\n",
      "\n",
      "Final (critiqued) response:\n",
      "I couldn't find any information on a programming language called \"Pythex\" created in 2019. It's possible that \"Pythex\" might not be a well-known or widely used programming language, or it might be a misnomer.\n",
      "\n",
      "However, I found information on \"Pythex\" as a regular expression testing and debugging tool, but I couldn't verify if it's a Python library or a standalone tool. If you're looking for information on regular expression testing and debugging, I can provide more general information on the topic.\n",
      "\n",
      "Note: The revised response is more accurate and cautious, and it avoids making claims that are not verifiable. It also provides a more general answer to the user's question, which is more helpful than providing potentially incorrect information.\n",
      "\n",
      "üìä Metrics: 888 tokens | 1369ms\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constitutional AI tests:   6%|‚ñã         | 1/16 [00:02<00:35,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Prompt: What are the main features of the XYZCrypt encryption algorithm?\n",
      "\n",
      "Initial response: I couldn't find any information on an encryption algorithm known as \"XYZCrypt.\" It's possible that it's a fictional or non-existent algorithm, or it m...\n",
      "\n",
      "Final (critiqued) response:\n",
      "I couldn't find any information on an encryption algorithm known as \"XYZCrypt.\" It appears that \"XYZCrypt\" is not a recognized or well-known encryption algorithm. If you could provide more context or clarify what you are referring to, I'd be happy to try and help you better. Alternatively, I can provide information on various well-known encryption algorithms, such as AES, RSA, or DES, if that's what you're looking for.\n",
      "\n",
      "üìä Metrics: 683 tokens | 1042ms\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constitutional AI tests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:40<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Constitutional AI testing complete!\n",
      "üìà Summary: 19039 total tokens | Avg 1190 tokens/test | Avg 5265ms/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chain-of-Thought Strategy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:33:00.539336Z",
     "start_time": "2025-11-08T11:32:16.650255Z"
    }
   },
   "source": "print(\"Testing Chain-of-Thought strategy...\\n\")\nprint(\"This prompts explicit reasoning and uncertainty markers.\\n\")\n\n# Track metrics\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, vector in enumerate(tqdm(test_set, desc=\"Chain-of-Thought tests\")):\n    prompt = vector['prompt']\n    \n    # Query with CoT\n    response, metadata = agent.query_with_chain_of_thought(prompt)\n    \n    # Track metrics\n    tokens = metadata.get('tokens_used', 0)\n    resp_time = metadata.get('response_time_ms', 0)\n    total_tokens += tokens\n    total_time += resp_time\n    \n    # Show example with metrics\n    if i < 2:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"Prompt: {prompt}\")\n        print(f\"\\nChain-of-Thought response:\\n{response}\")\n        print(f\"\\nüìä Metrics: {tokens} tokens | {resp_time:.0f}ms\")\n        print(\"=\"*80)\n    \n    # Annotate\n    is_hallucination = False\n    \n    # Log\n    db.log_test(\n        experiment_id=experiments['chain_of_thought'],\n        prompt_text=prompt,\n        response_text=response,\n        is_hallucination=is_hallucination,\n        prompt_category=vector['category'],\n        vector_type=vector.get('category', 'unknown'),\n        hallucination_type='none' if not is_hallucination else vector['category'],\n        severity=vector.get('severity', 'low'),\n        description=vector.get('description', ''),\n        response_time_ms=metadata.get('response_time_ms', 0),\n        tokens_used=metadata.get('tokens_used', 0)\n    )\n    \n    time.sleep(1)\n\n# Summary\navg_tokens = total_tokens / len(test_set) if len(test_set) > 0 else 0\navg_time = total_time / len(test_set) if len(test_set) > 0 else 0\nprint(f\"\\n‚úÖ Chain-of-Thought testing complete!\")\nprint(f\"üìà Summary: {total_tokens} total tokens | Avg {avg_tokens:.0f} tokens/test | Avg {avg_time:.0f}ms/test\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Chain-of-Thought strategy...\n",
      "\n",
      "This prompts explicit reasoning and uncertainty markers.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought tests:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Prompt: Tell me about the programming language \"Pythex\" created in 2019.\n",
      "\n",
      "Chain-of-Thought response:\n",
      "REASONING:\n",
      "\n",
      "1. Break down what the question is asking: The question is asking about a programming language called \"Pythex\" created in 2019. This means I need to find information about the language, its creation date, and any notable features or facts.\n",
      "\n",
      "2. Think through what I know about this topic: I'm a large language model, I have been trained on a vast amount of text data, including information about programming languages. However, I don't have any prior knowledge about a programming language called \"Pythex\" created in 2019. I'll have to search for this specific information.\n",
      "\n",
      "3. Identify any facts I'm uncertain about: I'm uncertain about the existence of the Pythex programming language, its features, and its creators.\n",
      "\n",
      "4. Provide my answer: Unfortunately, I couldn't find any information about a programming language called \"Pythex\" created in 2019. It's possible that it's a non-existent language, a private project, or a language that hasn't gained much popularity.\n",
      "\n",
      "ANSWER: I couldn't find any information about a programming language called \"Pythex\" created in 2019.\n",
      "\n",
      "CONFIDENCE: Medium\n",
      "LIMITATIONS: I'm uncertain about the existence of the Pythex programming language, its features, and its creators. I also couldn't find any information about it in my training data, which might indicate that it's a non-existent language or a language that hasn't been widely adopted or documented.\n",
      "\n",
      "üìä Metrics: 465 tokens | 715ms\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought tests:   6%|‚ñã         | 1/16 [00:01<00:25,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Prompt: What are the main features of the XYZCrypt encryption algorithm?\n",
      "\n",
      "Chain-of-Thought response:\n",
      "REASONING:\n",
      "\n",
      "1. **Breaking down the question**: The question is asking about the main features of the XYZCrypt encryption algorithm. This implies that I should be able to identify the key characteristics, components, or properties of the algorithm.\n",
      "\n",
      "2. **Thinking through what I know**: Unfortunately, I couldn't find any information about an encryption algorithm called \"XYZCrypt\" in my knowledge base. This suggests that XYZCrypt may be a fictional, unknown, or very obscure encryption algorithm.\n",
      "\n",
      "3. **Identifying any facts I'm uncertain about**: Given the lack of information, I am uncertain about the following facts:\n",
      "- **Existence**: Does XYZCrypt even exist as an encryption algorithm?\n",
      "- **Purpose**: What is the purpose of XYZCrypt (e.g., data encryption, secure communication, etc.)?\n",
      "- **Key features**: What are the main features or components of the algorithm?\n",
      "\n",
      "4. **Providing my answer**: Based on the lack of information, I will provide a generic answer with uncertain information marked.\n",
      "\n",
      "ANSWER: \n",
      "The XYZCrypt encryption algorithm is a fictional or unknown encryption algorithm with uncertain features. It may have a specific purpose, such as data encryption or secure communication, but this is not confirmed. Key features of the algorithm are unknown.\n",
      "\n",
      "CONFIDENCE: Low\n",
      "LIMITATIONS: \n",
      "- **Existence**: I couldn't find any information about an encryption algorithm called \"XYZCrypt\".\n",
      "- **Purpose**: The purpose of XYZCrypt is unknown.\n",
      "- **Key features**: The main features or components of the algorithm are uncertain.\n",
      "\n",
      "Please note that the lack of information about XYZCrypt makes it difficult to provide a more detailed or accurate answer. If you have any further information or context about XYZCrypt, I would be happy to try and provide a more informed response.\n",
      "\n",
      "üìä Metrics: 514 tokens | 802ms\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought tests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:43<00:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Chain-of-Thought testing complete!\n",
      "üìà Summary: 9095 total tokens | Avg 568 tokens/test | Avg 1724ms/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Now let's compare all strategies (including baseline from previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:33:26.728954Z",
     "start_time": "2025-11-08T11:33:26.603567Z"
    }
   },
   "source": [
    "# Get all experiments\n",
    "all_experiments = db.get_all_experiments()\n",
    "print(\"All Experiments:\")\n",
    "print(all_experiments)\n",
    "\n",
    "# Filter to mitigation strategies\n",
    "comparison = all_experiments[all_experiments['mitigation_strategy'].isin([\n",
    "    'baseline', 'rag', 'constitutional_ai', 'chain_of_thought'\n",
    "])].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['name', 'mitigation_strategy', 'total_tests', \n",
    "                  'hallucinations_detected', 'hallucination_rate']])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Experiments:\n",
      "    experiment_id                                               name  \\\n",
      "0              20  Comparative Analysis - RAG (Retrieval-Augmente...   \n",
      "1              21           Comparative Analysis - Constitutional AI   \n",
      "2              22  Comparative Analysis - Chain-of-Thought Verifi...   \n",
      "3              17  Comparative Analysis - RAG (Retrieval-Augmente...   \n",
      "4              18           Comparative Analysis - Constitutional AI   \n",
      "5              19  Comparative Analysis - Chain-of-Thought Verifi...   \n",
      "6              14  Comparative Analysis - RAG (Retrieval-Augmente...   \n",
      "7              15           Comparative Analysis - Constitutional AI   \n",
      "8              16  Comparative Analysis - Chain-of-Thought Verifi...   \n",
      "9              12            Unintentional Hallucinations - Baseline   \n",
      "10             13                           Control Tests - Baseline   \n",
      "11             10            Unintentional Hallucinations - Baseline   \n",
      "12             11                           Control Tests - Baseline   \n",
      "13              8            Unintentional Hallucinations - Baseline   \n",
      "14              9                           Control Tests - Baseline   \n",
      "15              6            Unintentional Hallucinations - Baseline   \n",
      "16              7                           Control Tests - Baseline   \n",
      "17              4            Unintentional Hallucinations - Baseline   \n",
      "18              5                           Control Tests - Baseline   \n",
      "19              2            Unintentional Hallucinations - Baseline   \n",
      "20              3                           Control Tests - Baseline   \n",
      "21              1              Intentional Hallucinations - Baseline   \n",
      "\n",
      "   mitigation_strategy           created_at  total_tests  \\\n",
      "0                  rag  2025-11-08 11:29:45           16   \n",
      "1    constitutional_ai  2025-11-08 11:29:45           16   \n",
      "2     chain_of_thought  2025-11-08 11:29:45           16   \n",
      "3                  rag  2025-11-08 11:08:36           16   \n",
      "4    constitutional_ai  2025-11-08 11:08:36           16   \n",
      "5     chain_of_thought  2025-11-08 11:08:36           16   \n",
      "6                  rag  2025-11-08 03:43:18           16   \n",
      "7    constitutional_ai  2025-11-08 03:43:18           16   \n",
      "8     chain_of_thought  2025-11-08 03:43:18           16   \n",
      "9             baseline  2025-11-08 03:38:52            0   \n",
      "10            baseline  2025-11-08 03:38:52            0   \n",
      "11            baseline  2025-11-08 03:21:23           16   \n",
      "12            baseline  2025-11-08 03:21:23            5   \n",
      "13            baseline  2025-11-08 02:38:07            0   \n",
      "14            baseline  2025-11-08 02:38:07            0   \n",
      "15            baseline  2025-11-08 02:37:45            0   \n",
      "16            baseline  2025-11-08 02:37:45            0   \n",
      "17            baseline  2025-11-08 02:13:47           32   \n",
      "18            baseline  2025-11-08 02:13:47            5   \n",
      "19            baseline  2025-11-02 07:53:35            0   \n",
      "20            baseline  2025-11-02 07:53:35            0   \n",
      "21            baseline  2025-11-02 07:38:17           16   \n",
      "\n",
      "    hallucinations_detected hallucination_rate  \n",
      "0                         0              0.00%  \n",
      "1                         0              0.00%  \n",
      "2                         0              0.00%  \n",
      "3                         0              0.00%  \n",
      "4                         0              0.00%  \n",
      "5                         0              0.00%  \n",
      "6                         0              0.00%  \n",
      "7                         0              0.00%  \n",
      "8                         0              0.00%  \n",
      "9                         0              0.00%  \n",
      "10                        0              0.00%  \n",
      "11                        0              0.00%  \n",
      "12                        0              0.00%  \n",
      "13                        0              0.00%  \n",
      "14                        0              0.00%  \n",
      "15                        0              0.00%  \n",
      "16                        0              0.00%  \n",
      "17                        0              0.00%  \n",
      "18                        0              0.00%  \n",
      "19                        0              0.00%  \n",
      "20                        0              0.00%  \n",
      "21                       16            100.00%  \n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE RESULTS\n",
      "================================================================================\n",
      "                                                 name mitigation_strategy  \\\n",
      "0   Comparative Analysis - RAG (Retrieval-Augmente...                 rag   \n",
      "1            Comparative Analysis - Constitutional AI   constitutional_ai   \n",
      "2   Comparative Analysis - Chain-of-Thought Verifi...    chain_of_thought   \n",
      "3   Comparative Analysis - RAG (Retrieval-Augmente...                 rag   \n",
      "4            Comparative Analysis - Constitutional AI   constitutional_ai   \n",
      "5   Comparative Analysis - Chain-of-Thought Verifi...    chain_of_thought   \n",
      "6   Comparative Analysis - RAG (Retrieval-Augmente...                 rag   \n",
      "7            Comparative Analysis - Constitutional AI   constitutional_ai   \n",
      "8   Comparative Analysis - Chain-of-Thought Verifi...    chain_of_thought   \n",
      "9             Unintentional Hallucinations - Baseline            baseline   \n",
      "10                           Control Tests - Baseline            baseline   \n",
      "11            Unintentional Hallucinations - Baseline            baseline   \n",
      "12                           Control Tests - Baseline            baseline   \n",
      "13            Unintentional Hallucinations - Baseline            baseline   \n",
      "14                           Control Tests - Baseline            baseline   \n",
      "15            Unintentional Hallucinations - Baseline            baseline   \n",
      "16                           Control Tests - Baseline            baseline   \n",
      "17            Unintentional Hallucinations - Baseline            baseline   \n",
      "18                           Control Tests - Baseline            baseline   \n",
      "19            Unintentional Hallucinations - Baseline            baseline   \n",
      "20                           Control Tests - Baseline            baseline   \n",
      "21              Intentional Hallucinations - Baseline            baseline   \n",
      "\n",
      "    total_tests  hallucinations_detected hallucination_rate  \n",
      "0            16                        0              0.00%  \n",
      "1            16                        0              0.00%  \n",
      "2            16                        0              0.00%  \n",
      "3            16                        0              0.00%  \n",
      "4            16                        0              0.00%  \n",
      "5            16                        0              0.00%  \n",
      "6            16                        0              0.00%  \n",
      "7            16                        0              0.00%  \n",
      "8            16                        0              0.00%  \n",
      "9             0                        0              0.00%  \n",
      "10            0                        0              0.00%  \n",
      "11           16                        0              0.00%  \n",
      "12            5                        0              0.00%  \n",
      "13            0                        0              0.00%  \n",
      "14            0                        0              0.00%  \n",
      "15            0                        0              0.00%  \n",
      "16            0                        0              0.00%  \n",
      "17           32                        0              0.00%  \n",
      "18            5                        0              0.00%  \n",
      "19            0                        0              0.00%  \n",
      "20            0                        0              0.00%  \n",
      "21           16                       16            100.00%  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:33:38.602821Z",
     "start_time": "2025-11-08T11:33:38.495026Z"
    }
   },
   "source": "# Detailed comparison with beautiful formatting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Prepare data - get the LATEST experiment for each strategy\nstrategy_stats = []\nfor strategy in ['baseline', 'rag', 'constitutional_ai', 'chain_of_thought']:\n    exp = comparison[comparison['mitigation_strategy'] == strategy]\n    if len(exp) > 0:\n        # Get the MOST RECENT experiment for this strategy\n        exp_id = exp.sort_values('created_at', ascending=False).iloc[0]['experiment_id']\n        df = db.get_experiment_results(exp_id)\n        \n        if len(df) > 0:  # Only if we have data\n            # Ensure numeric types and handle missing data\n            df['response_time_ms'] = pd.to_numeric(df['response_time_ms'], errors='coerce').fillna(0)\n            df['tokens_used'] = pd.to_numeric(df['tokens_used'], errors='coerce').fillna(0)\n            \n            strategy_stats.append({\n                'Strategy': strategy.replace('_', ' ').title(),\n                'Tests': len(df),\n                'Hallucinations': int(df['is_hallucination'].sum()),\n                'Accuracy': f\"{(1 - df['is_hallucination'].mean()) * 100:.1f}%\",\n                'Avg Time (ms)': f\"{df['response_time_ms'].mean():.0f}\",\n                'Avg Tokens': f\"{df['tokens_used'].mean():.0f}\",\n                # Keep numeric versions for plotting\n                '_accuracy_num': (1 - df['is_hallucination'].mean()) * 100,\n                '_time_num': df['response_time_ms'].mean(),\n                '_tokens_num': df['tokens_used'].mean()\n            })\n\ndf_stats = pd.DataFrame(strategy_stats)\n\n# Create beautiful styled table\nprint(\"\\n\" + \"=\"*90)\nprint(\"üìä COMPARATIVE STRATEGY ANALYSIS - DETAILED METRICS\")\nprint(\"=\"*90 + \"\\n\")\n\n# Display as HTML table with styling\nif len(df_stats) > 0:\n    html_table = \"\"\"\n    <style>\n        .dataframe-container {\n            font-family: 'Segoe UI', Arial, sans-serif;\n            margin: 20px 0;\n        }\n        .results-table {\n            border-collapse: collapse;\n            width: 100%;\n            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n            border-radius: 8px;\n            overflow: hidden;\n        }\n        .results-table th {\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            color: white;\n            padding: 15px;\n            text-align: left;\n            font-weight: 600;\n            font-size: 13px;\n            text-transform: uppercase;\n            letter-spacing: 0.5px;\n        }\n        .results-table td {\n            padding: 12px 15px;\n            border-bottom: 1px solid #e0e0e0;\n            font-size: 13px;\n        }\n        .results-table tr:nth-child(even) {\n            background-color: #f8f9fa;\n        }\n        .results-table tr:hover {\n            background-color: #e3f2fd;\n            transition: background-color 0.3s ease;\n        }\n        .metric-badge {\n            display: inline-block;\n            padding: 4px 10px;\n            border-radius: 12px;\n            font-weight: 600;\n            font-size: 12px;\n        }\n        .badge-success {\n            background-color: #d4edda;\n            color: #155724;\n        }\n        .badge-warning {\n            background-color: #fff3cd;\n            color: #856404;\n        }\n        .badge-info {\n            background-color: #d1ecf1;\n            color: #0c5460;\n        }\n    </style>\n    <div class=\"dataframe-container\">\n        <table class=\"results-table\">\n            <thead>\n                <tr>\n                    <th>Strategy</th>\n                    <th>Tests Run</th>\n                    <th>Hallucinations</th>\n                    <th>Accuracy</th>\n                    <th>Avg Response Time</th>\n                    <th>Avg Tokens Used</th>\n                </tr>\n            </thead>\n            <tbody>\n    \"\"\"\n    \n    for _, row in df_stats.iterrows():\n        accuracy_val = float(row['Accuracy'].rstrip('%'))\n        badge_class = 'badge-success' if accuracy_val >= 90 else ('badge-warning' if accuracy_val >= 70 else 'badge-info')\n        \n        html_table += f\"\"\"\n                <tr>\n                    <td><strong>{row['Strategy']}</strong></td>\n                    <td>{row['Tests']}</td>\n                    <td>{row['Hallucinations']}</td>\n                    <td><span class=\"metric-badge {badge_class}\">{row['Accuracy']}</span></td>\n                    <td>{row['Avg Time (ms)']} ms</td>\n                    <td>{row['Avg Tokens']}</td>\n                </tr>\n        \"\"\"\n    \n    html_table += \"\"\"\n            </tbody>\n        </table>\n    </div>\n    \"\"\"\n    \n    display(HTML(html_table))\nelse:\n    print(\"‚ö†Ô∏è  No data available for comparison. Please ensure experiments have been run.\")\n\nprint(\"\\n\" + \"=\"*90)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "üìä COMPARATIVE STRATEGY ANALYSIS - DETAILED METRICS\n",
      "==========================================================================================\n",
      "\n",
      "‚ö†Ô∏è  No data available for comparison. Please ensure experiments have been run.\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T11:33:47.121038Z",
     "start_time": "2025-11-08T11:33:47.065792Z"
    }
   },
   "source": "# Professional, Modern Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Set professional style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = '#f8f9fa'\nplt.rcParams['font.family'] = 'sans-serif'\n\nif len(df_stats) > 0 and '_accuracy_num' in df_stats.columns:\n    # Create figure with subplots\n    fig = plt.figure(figsize=(20, 12))\n    gs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.3)\n    \n    # Color palette - modern and professional\n    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n    accent_color = '#2c3e50'\n    \n    # 1. Accuracy Comparison (Horizontal Bar)\n    ax1 = fig.add_subplot(gs[0, 0])\n    y_pos = np.arange(len(df_stats))\n    bars1 = ax1.barh(y_pos, df_stats['_accuracy_num'], \n                     color=colors[:len(df_stats)], alpha=0.85, edgecolor=accent_color, linewidth=2)\n    ax1.set_yticks(y_pos)\n    ax1.set_yticklabels(df_stats['Strategy'], fontsize=11, fontweight='600')\n    ax1.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    ax1.set_title('üéØ Accuracy by Strategy', fontsize=14, fontweight='bold', pad=15, color=accent_color)\n    ax1.set_xlim(0, 105)\n    ax1.grid(axis='x', alpha=0.3, linestyle='--')\n    \n    # Add value labels\n    for i, (bar, val) in enumerate(zip(bars1, df_stats['_accuracy_num'])):\n        ax1.text(val + 1, bar.get_y() + bar.get_height()/2, \n                f'{val:.1f}%', va='center', fontweight='bold', fontsize=10)\n    \n    # 2. Tokens vs Accuracy Scatter\n    ax2 = fig.add_subplot(gs[0, 1])\n    sizes = [300, 400, 500, 600][:len(df_stats)]\n    for idx, (_, row) in enumerate(df_stats.iterrows()):\n        ax2.scatter(row['_tokens_num'], row['_accuracy_num'], \n                   s=sizes[idx], c=[colors[idx]], alpha=0.6, \n                   edgecolors=accent_color, linewidth=2.5, zorder=3)\n        ax2.annotate(row['Strategy'], \n                    (row['_tokens_num'], row['_accuracy_num']),\n                    xytext=(10, 10), textcoords='offset points',\n                    fontsize=10, fontweight='600',\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', \n                             edgecolor=colors[idx], alpha=0.9, linewidth=2))\n    \n    ax2.set_xlabel('Average Tokens Used', fontsize=12, fontweight='bold')\n    ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    ax2.set_title('üí∞ Cost vs Accuracy Trade-off', fontsize=14, fontweight='bold', pad=15, color=accent_color)\n    ax2.grid(True, alpha=0.3, linestyle='--')\n    ax2.set_ylim(bottom=max(0, df_stats['_accuracy_num'].min() - 10))\n    \n    # 3. Response Time vs Accuracy Scatter\n    ax3 = fig.add_subplot(gs[0, 2])\n    for idx, (_, row) in enumerate(df_stats.iterrows()):\n        ax3.scatter(row['_time_num'], row['_accuracy_num'], \n                   s=sizes[idx], c=[colors[idx]], alpha=0.6,\n                   edgecolors=accent_color, linewidth=2.5, zorder=3)\n        ax3.annotate(row['Strategy'],\n                    (row['_time_num'], row['_accuracy_num']),\n                    xytext=(10, 10), textcoords='offset points',\n                    fontsize=10, fontweight='600',\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white',\n                             edgecolor=colors[idx], alpha=0.9, linewidth=2))\n    \n    ax3.set_xlabel('Average Response Time (ms)', fontsize=12, fontweight='bold')\n    ax3.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    ax3.set_title('‚ö° Speed vs Accuracy Trade-off', fontsize=14, fontweight='bold', pad=15, color=accent_color)\n    ax3.grid(True, alpha=0.3, linestyle='--')\n    ax3.set_ylim(bottom=max(0, df_stats['_accuracy_num'].min() - 10))\n    \n    # 4. Combined Metrics (Radar-like comparison)\n    ax4 = fig.add_subplot(gs[1, :])\n    x = np.arange(len(df_stats))\n    width = 0.25\n    \n    # Normalize metrics for comparison\n    norm_acc = df_stats['_accuracy_num'] / 100\n    norm_tokens = 1 - (df_stats['_tokens_num'] / df_stats['_tokens_num'].max()) if df_stats['_tokens_num'].max() > 0 else [0]*len(df_stats)\n    norm_time = 1 - (df_stats['_time_num'] / df_stats['_time_num'].max()) if df_stats['_time_num'].max() > 0 else [0]*len(df_stats)\n    \n    bars_acc = ax4.bar(x - width, norm_acc, width, label='Accuracy', \n                      color='#2ecc71', alpha=0.85, edgecolor=accent_color, linewidth=1.5)\n    bars_cost = ax4.bar(x, norm_tokens, width, label='Cost Efficiency', \n                       color='#3498db', alpha=0.85, edgecolor=accent_color, linewidth=1.5)\n    bars_speed = ax4.bar(x + width, norm_time, width, label='Speed', \n                        color='#f39c12', alpha=0.85, edgecolor=accent_color, linewidth=1.5)\n    \n    ax4.set_xlabel('Strategy', fontsize=13, fontweight='bold')\n    ax4.set_ylabel('Normalized Score (0-1)', fontsize=13, fontweight='bold')\n    ax4.set_title('üìä Overall Performance Comparison (Normalized Metrics)', \n                 fontsize=15, fontweight='bold', pad=20, color=accent_color)\n    ax4.set_xticks(x)\n    ax4.set_xticklabels(df_stats['Strategy'], fontsize=11, fontweight='600')\n    ax4.legend(loc='upper left', frameon=True, shadow=True, fontsize=11)\n    ax4.set_ylim(0, 1.1)\n    ax4.grid(axis='y', alpha=0.3, linestyle='--')\n    ax4.set_axisbelow(True)\n    \n    # Add value labels on bars\n    for bars in [bars_acc, bars_cost, bars_speed]:\n        for bar in bars:\n            height = bar.get_height()\n            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n                    f'{height:.2f}', ha='center', va='bottom', \n                    fontsize=9, fontweight='bold')\n    \n    plt.suptitle('Mitigation Strategy Performance Analysis', \n                fontsize=18, fontweight='bold', y=0.995, color=accent_color)\n    \n    plt.savefig('../results/charts/strategy_comparison.png', \n                dpi=300, bbox_inches='tight', facecolor='white')\n    plt.show()\n    \n    print(\"‚úÖ Professional visualizations saved to results/charts/strategy_comparison.png\")\nelse:\n    print(\"‚ö†Ô∏è  Insufficient data for visualization. Please run the experiments first.\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Insufficient data for visualization. Please run the experiments first.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Document your analysis:**\n",
    "\n",
    "1. **Most Effective Strategy:**\n",
    "   - Which strategy had the lowest hallucination rate?\n",
    "   - Was the reduction significant?\n",
    "\n",
    "2. **Trade-offs:**\n",
    "   - Which strategy used the most tokens (cost)?\n",
    "   - Which was fastest?\n",
    "   - Is the accuracy improvement worth the cost?\n",
    "\n",
    "3. **Scenario-Specific Performance:**\n",
    "   - Did certain strategies work better for specific types of prompts?\n",
    "   - RAG performance on factual vs. speculative questions?\n",
    "\n",
    "4. **Practical Recommendations:**\n",
    "   - When would you use each strategy?\n",
    "   - Could you combine strategies?\n",
    "\n",
    "**Your analysis:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **04_data_analysis_visualization.ipynb** for comprehensive data analysis and visualizations for your report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "db.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
