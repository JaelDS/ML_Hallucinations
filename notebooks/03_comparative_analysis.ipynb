{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Mitigation Strategy Analysis\n",
    "\n",
    "This notebook compares the effectiveness of different hallucination mitigation strategies:\n",
    "\n",
    "1. **Baseline** - No mitigation (already tested)\n",
    "2. **RAG** - Retrieval-Augmented Generation with curated knowledge base\n",
    "3. **Constitutional AI** - Self-critique and refinement\n",
    "4. **Chain-of-Thought** - Step-by-step reasoning with uncertainty markers\n",
    "\n",
    "## Objectives\n",
    "- Test each strategy on the same prompts\n",
    "- Measure hallucination reduction\n",
    "- Compare cost (tokens), speed, and accuracy\n",
    "- Identify which strategy works best for which scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from agent import HallucinationTestAgent\n",
    "from database import HallucinationDB\n",
    "from test_vectors import HallucinationTestVectors\n",
    "from rag_utils import create_default_knowledge_base\n",
    "from config import Config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "agent = HallucinationTestAgent()\n",
    "db = HallucinationDB()\n",
    "kb = create_default_knowledge_base()\n",
    "\n",
    "print(\"✓ Agent initialized\")\n",
    "print(f\"✓ Knowledge base loaded: {kb.get_count()} documents\")\n",
    "print(f\"✓ Database ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Test Vectors\n",
    "\n",
    "We'll use a representative sample from each category for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all vectors\n",
    "all_vectors = HallucinationTestVectors.get_all_vectors()\n",
    "\n",
    "# Create combined test set (sample from each type)\n",
    "test_set = [\n",
    "    # High-risk intentional vectors (should hallucinate in baseline)\n",
    "    *all_vectors['intentional'][:8],  # First 8 intentional\n",
    "    # Edge cases\n",
    "    *all_vectors['unintentional'][:5],  # First 5 unintentional\n",
    "    # Control (should NOT hallucinate in any strategy)\n",
    "    *all_vectors['control'][:3]  # First 3 control\n",
    "]\n",
    "\n",
    "print(f\"Test set size: {len(test_set)} prompts\")\n",
    "print(\"\\nBreakdown:\")\n",
    "for vector_type in ['intentional', 'unintentional', 'control']:\n",
    "    count = sum(1 for v in test_set if v.get('category') in \n",
    "                [vec['category'] for vec in all_vectors[vector_type]])\n",
    "    print(f\"  {vector_type}: ~{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiments for Each Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment IDs for each mitigation strategy\n",
    "experiments = {}\n",
    "\n",
    "strategies = [\n",
    "    ('rag', 'RAG (Retrieval-Augmented Generation)', \n",
    "     'Testing with curated cybersecurity knowledge base for grounding'),\n",
    "    ('constitutional_ai', 'Constitutional AI', \n",
    "     'Testing with self-critique and constitutional principles'),\n",
    "    ('chain_of_thought', 'Chain-of-Thought Verification', \n",
    "     'Testing with step-by-step reasoning and uncertainty markers')\n",
    "]\n",
    "\n",
    "for strategy_key, strategy_name, description in strategies:\n",
    "    exp_id = db.create_experiment(\n",
    "        name=f\"Comparative Analysis - {strategy_name}\",\n",
    "        mitigation_strategy=strategy_key,\n",
    "        description=description\n",
    "    )\n",
    "    experiments[strategy_key] = exp_id\n",
    "    print(f\"✓ {strategy_name}: Experiment ID {exp_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RAG strategy...\\n\")\n",
    "print(\"This retrieves relevant documents before answering.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_set, desc=\"RAG tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Retrieve relevant context\n",
    "    context_docs, scores = kb.query(prompt, n_results=3)\n",
    "    \n",
    "    # Query with RAG\n",
    "    response, metadata = agent.query_with_rag(prompt, context_docs)\n",
    "    \n",
    "    # Show example\n",
    "    if i < 2:  # Show first 2\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nRetrieved context (top document):\")\n",
    "        print(f\"{context_docs[0][:150]}...\")\n",
    "        print(f\"\\nRAG Response:\\n{response}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Annotate (automated for demonstration)\n",
    "    # In real scenario: manual review needed\n",
    "    is_hallucination = False  # RAG should reduce hallucinations\n",
    "    \n",
    "    # Log\n",
    "    db.log_test(\n",
    "        experiment_id=experiments['rag'],\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type=vector.get('category', 'unknown'),\n",
    "        hallucination_type='none' if not is_hallucination else vector['category'],\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector.get('description', ''),\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0),\n",
    "        retrieved_documents=str(context_docs),\n",
    "        num_documents=len(context_docs)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ RAG testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Constitutional AI Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Constitutional AI strategy...\\n\")\n",
    "print(\"This uses self-critique to identify and fix hallucinations.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_set, desc=\"Constitutional AI tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query with Constitutional AI\n",
    "    response, metadata = agent.query_with_constitutional_ai(prompt)\n",
    "    \n",
    "    # Show example\n",
    "    if i < 2:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nInitial response: {metadata.get('initial_response', 'N/A')[:150]}...\")\n",
    "        print(f\"\\nFinal (critiqued) response:\\n{response}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Annotate\n",
    "    is_hallucination = False\n",
    "    \n",
    "    # Log\n",
    "    db.log_test(\n",
    "        experiment_id=experiments['constitutional_ai'],\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type=vector.get('category', 'unknown'),\n",
    "        hallucination_type='none' if not is_hallucination else vector['category'],\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector.get('description', ''),\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ Constitutional AI testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chain-of-Thought Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Chain-of-Thought strategy...\\n\")\n",
    "print(\"This prompts explicit reasoning and uncertainty markers.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_set, desc=\"Chain-of-Thought tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query with CoT\n",
    "    response, metadata = agent.query_with_chain_of_thought(prompt)\n",
    "    \n",
    "    # Show example\n",
    "    if i < 2:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"\\nChain-of-Thought response:\\n{response}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Annotate\n",
    "    is_hallucination = False\n",
    "    \n",
    "    # Log\n",
    "    db.log_test(\n",
    "        experiment_id=experiments['chain_of_thought'],\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type=vector.get('category', 'unknown'),\n",
    "        hallucination_type='none' if not is_hallucination else vector['category'],\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector.get('description', ''),\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ Chain-of-Thought testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Now let's compare all strategies (including baseline from previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all experiments\n",
    "all_experiments = db.get_all_experiments()\n",
    "print(\"All Experiments:\")\n",
    "print(all_experiments)\n",
    "\n",
    "# Filter to mitigation strategies\n",
    "comparison = all_experiments[all_experiments['mitigation_strategy'].isin([\n",
    "    'baseline', 'rag', 'constitutional_ai', 'chain_of_thought'\n",
    "])].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['name', 'mitigation_strategy', 'total_tests', \n",
    "                  'hallucinations_detected', 'hallucination_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detailed comparison\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Prepare data\nstrategy_stats = []\nfor strategy in ['baseline', 'rag', 'constitutional_ai', 'chain_of_thought']:\n    exp = comparison[comparison['mitigation_strategy'] == strategy]\n    if len(exp) > 0:\n        # Get first matching experiment\n        exp_id = exp.iloc[0]['experiment_id']\n        df = db.get_experiment_results(exp_id)\n        \n        # Ensure numeric types and handle missing data\n        df['response_time_ms'] = pd.to_numeric(df['response_time_ms'], errors='coerce').fillna(0)\n        df['tokens_used'] = pd.to_numeric(df['tokens_used'], errors='coerce').fillna(0)\n        \n        strategy_stats.append({\n            'Strategy': strategy.replace('_', ' ').title(),\n            'Hallucination Rate': df['is_hallucination'].mean() * 100,\n            'Avg Response Time (ms)': df['response_time_ms'].mean(),\n            'Avg Tokens': df['tokens_used'].mean(),\n            'Total Tests': len(df)\n        })\n\ndf_stats = pd.DataFrame(strategy_stats)\nprint(\"\\nDetailed Strategy Statistics:\")\nprint(df_stats)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Professional Visualization with Modern Styling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set modern, minimal style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nfig = plt.figure(figsize=(18, 6))\ngs = fig.add_gridspec(1, 3, hspace=0.3, wspace=0.3)\n\n# Modern color palette\ncolors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n\n# 1. Hallucination Rate\nax1 = fig.add_subplot(gs[0, 0])\nbars1 = ax1.bar(range(len(df_stats)), df_stats['Hallucination Rate'], \n                color=colors[:len(df_stats)], alpha=0.8, edgecolor='black', linewidth=1.5)\nax1.set_xticks(range(len(df_stats)))\nax1.set_xticklabels(df_stats['Strategy'], rotation=25, ha='right', fontsize=11)\nax1.set_ylabel('Hallucination Rate (%)', fontsize=13, fontweight='bold')\nax1.set_title('Hallucination Rate by Strategy', fontsize=14, fontweight='bold', pad=15)\nax1.grid(axis='y', alpha=0.3, linestyle='--')\nax1.set_axisbelow(True)\n\n# Add value labels\nfor i, (bar, val) in enumerate(zip(bars1, df_stats['Hallucination Rate'])):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n            f'{val:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n\n# 2. Response Time\nax2 = fig.add_subplot(gs[0, 1])\nbars2 = ax2.bar(range(len(df_stats)), df_stats['Avg Response Time (ms)'], \n                color=colors[:len(df_stats)], alpha=0.8, edgecolor='black', linewidth=1.5)\nax2.set_xticks(range(len(df_stats)))\nax2.set_xticklabels(df_stats['Strategy'], rotation=25, ha='right', fontsize=11)\nax2.set_ylabel('Response Time (ms)', fontsize=13, fontweight='bold')\nax2.set_title('Average Response Time', fontsize=14, fontweight='bold', pad=15)\nax2.grid(axis='y', alpha=0.3, linestyle='--')\nax2.set_axisbelow(True)\n\n# Add value labels\nfor i, (bar, val) in enumerate(zip(bars2, df_stats['Avg Response Time (ms)'])):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height + max(df_stats['Avg Response Time (ms)'])*0.02,\n            f'{val:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n\n# 3. Token Usage\nax3 = fig.add_subplot(gs[0, 2])\nbars3 = ax3.bar(range(len(df_stats)), df_stats['Avg Tokens'], \n                color=colors[:len(df_stats)], alpha=0.8, edgecolor='black', linewidth=1.5)\nax3.set_xticks(range(len(df_stats)))\nax3.set_xticklabels(df_stats['Strategy'], rotation=25, ha='right', fontsize=11)\nax3.set_ylabel('Tokens Used', fontsize=13, fontweight='bold')\nax3.set_title('Average Token Usage', fontsize=14, fontweight='bold', pad=15)\nax3.grid(axis='y', alpha=0.3, linestyle='--')\nax3.set_axisbelow(True)\n\n# Add value labels\nfor i, (bar, val) in enumerate(zip(bars3, df_stats['Avg Tokens'])):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height + max(df_stats['Avg Tokens'])*0.02,\n            f'{val:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n\nplt.suptitle('Comparative Strategy Analysis', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig('../results/charts/strategy_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"✓ Chart saved to results/charts/strategy_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Document your analysis:**\n",
    "\n",
    "1. **Most Effective Strategy:**\n",
    "   - Which strategy had the lowest hallucination rate?\n",
    "   - Was the reduction significant?\n",
    "\n",
    "2. **Trade-offs:**\n",
    "   - Which strategy used the most tokens (cost)?\n",
    "   - Which was fastest?\n",
    "   - Is the accuracy improvement worth the cost?\n",
    "\n",
    "3. **Scenario-Specific Performance:**\n",
    "   - Did certain strategies work better for specific types of prompts?\n",
    "   - RAG performance on factual vs. speculative questions?\n",
    "\n",
    "4. **Practical Recommendations:**\n",
    "   - When would you use each strategy?\n",
    "   - Could you combine strategies?\n",
    "\n",
    "**Your analysis:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **04_data_analysis_visualization.ipynb** for comprehensive data analysis and visualizations for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}