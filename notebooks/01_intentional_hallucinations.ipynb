{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intentional Hallucination Testing\n",
    "\n",
    "This notebook tests the model with prompts **designed to induce hallucinations**.\n",
    "\n",
    "## Objectives\n",
    "1. Test model responses to fabricated entities (fake CVEs, tools, papers)\n",
    "2. Document hallucination patterns and severity\n",
    "3. Build baseline dataset for mitigation comparison\n",
    "\n",
    "## Test Categories\n",
    "- Fabricated CVEs and security vulnerabilities\n",
    "- Non-existent tools and frameworks\n",
    "- Fake academic citations\n",
    "- Temporal impossibilities\n",
    "- Technical confabulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from agent import HallucinationTestAgent\n",
    "from database import HallucinationDB\n",
    "from test_vectors import HallucinationTestVectors\n",
    "from config import Config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent and database\n",
    "agent = HallucinationTestAgent()\n",
    "db = HallucinationDB()\n",
    "\n",
    "# Create experiment\n",
    "experiment_id = db.create_experiment(\n",
    "    name=\"Intentional Hallucinations - Baseline\",\n",
    "    mitigation_strategy=\"baseline\",\n",
    "    description=\"Testing model with intentionally hallucination-inducing prompts. No mitigation applied.\"\n",
    ")\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Model: {Config.MODEL_NAME}\")\n",
    "print(f\"Temperature: {Config.TEMPERATURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intentional hallucination test vectors\n",
    "test_vectors = HallucinationTestVectors.get_intentional_vectors()\n",
    "\n",
    "print(f\"Total test vectors: {len(test_vectors)}\")\n",
    "print(\"\\nTest categories:\")\n",
    "categories = {}\n",
    "for vec in test_vectors:\n",
    "    cat = vec['category']\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "for cat, count in sorted(categories.items()):\n",
    "    print(f\"  - {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Baseline Tests\n",
    "\n",
    "**IMPORTANT:** This will make API calls to OpenAI. Monitor your usage.\n",
    "\n",
    "Each test:\n",
    "1. Sends prompt to model\n",
    "2. Records response\n",
    "3. Requires manual annotation (you'll mark if hallucination occurred)\n",
    "4. Saves to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "results = []\n",
    "\n",
    "print(\"Starting baseline tests...\\n\")\n",
    "print(\"For each response, you'll be asked to confirm if it's a hallucination.\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(test_vectors, desc=\"Testing\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query model\n",
    "    response, metadata = agent.query_baseline(prompt)\n",
    "    \n",
    "    # Display for annotation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Test {i+1}/{len(test_vectors)}\")\n",
    "    print(f\"Category: {vector['category']}\")\n",
    "    print(f\"Expected: {vector['expected_hallucination']}\")\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"\\nResponse:\\n{response}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Manual annotation (in real scenario)\n",
    "    # For automated run, we'll use expected_hallucination as proxy\n",
    "    is_hallucination = vector['expected_hallucination']\n",
    "    \n",
    "    # Uncomment below for manual annotation:\n",
    "    # annotation = input(\"\\nIs this a hallucination? (y/n): \").strip().lower()\n",
    "    # is_hallucination = annotation == 'y'\n",
    "    \n",
    "    # Log to database\n",
    "    log_result = db.log_test(\n",
    "        experiment_id=experiment_id,\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type='intentional',\n",
    "        expected_hallucination=vector['expected_hallucination'],\n",
    "        hallucination_type=vector['category'],\n",
    "        severity=vector['severity'],\n",
    "        description=vector['description'],\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'is_hallucination': is_hallucination,\n",
    "        'category': vector['category'],\n",
    "        'tokens': metadata.get('tokens_used', 0)\n",
    "    })\n",
    "    \n",
    "    # Rate limiting - be nice to the API\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\nâœ“ Baseline testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from database\n",
    "df_results = db.get_experiment_results(experiment_id)\n",
    "\n",
    "print(\"Experiment Results Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total tests: {len(df_results)}\")\n",
    "print(f\"Hallucinations detected: {df_results['is_hallucination'].sum()}\")\n",
    "print(f\"Hallucination rate: {df_results['is_hallucination'].mean()*100:.1f}%\")\n",
    "print(f\"\\nAverage response time: {df_results['response_time_ms'].mean():.0f}ms\")\n",
    "print(f\"Total tokens used: {df_results['tokens_used'].sum()}\")\n",
    "\n",
    "print(\"\\nHallucinations by category:\")\n",
    "category_stats = df_results.groupby('prompt_category').agg({\n",
    "    'is_hallucination': ['count', 'sum', 'mean']\n",
    "}).round(3)\n",
    "category_stats.columns = ['Total', 'Hallucinations', 'Rate']\n",
    "print(category_stats)\n",
    "\n",
    "print(\"\\nSeverity distribution:\")\n",
    "print(df_results['severity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "export_path = db.export_to_csv(experiment_id)\n",
    "print(f\"Results exported to: {export_path}\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "display_cols = ['prompt_text', 'response_text', 'is_hallucination', 'hallucination_type', 'severity']\n",
    "df_results[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Document your observations here:**\n",
    "\n",
    "1. Which categories had highest hallucination rates?\n",
    "2. Were any responses surprisingly accurate or surprisingly fabricated?\n",
    "3. What patterns did you notice in how the model fabricates information?\n",
    "4. Did the model show any uncertainty markers (\"I'm not sure\", \"I don't have information\")?\n",
    "\n",
    "**Your notes:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have baseline data:\n",
    "\n",
    "1. Proceed to **02_unintentional_hallucinations.ipynb** to test edge cases\n",
    "2. Then test mitigation strategies in **03_comparative_analysis.ipynb**\n",
    "3. Compare how RAG, Constitutional AI, and Chain-of-Thought affect these same prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "db.close()\n",
    "print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
