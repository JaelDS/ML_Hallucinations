{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unintentional Hallucination Testing\n",
    "\n",
    "This notebook tests **edge cases and scenarios** where hallucinations may occur unintentionally.\n",
    "\n",
    "## Objectives\n",
    "1. Test knowledge boundaries (cutoff dates, obscure topics)\n",
    "2. Identify ambiguous queries that trigger fabrication\n",
    "3. Document statistical claims without sources\n",
    "4. Establish control baseline with well-known facts\n",
    "\n",
    "## Test Categories\n",
    "- Knowledge cutoff issues\n",
    "- Ambiguous/underspecified queries\n",
    "- Obscure but real topics\n",
    "- Statistical/numerical claims\n",
    "- Speculative future questions\n",
    "- Control questions (should NOT hallucinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from agent import HallucinationTestAgent\n",
    "from database import HallucinationDB\n",
    "from test_vectors import HallucinationTestVectors\n",
    "from config import Config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "agent = HallucinationTestAgent()\n",
    "db = HallucinationDB()\n",
    "\n",
    "# Create experiment for unintentional tests\n",
    "unintentional_exp_id = db.create_experiment(\n",
    "    name=\"Unintentional Hallucinations - Baseline\",\n",
    "    mitigation_strategy=\"baseline\",\n",
    "    description=\"Testing edge cases and knowledge boundaries. Unintentional hallucination scenarios.\"\n",
    ")\n",
    "\n",
    "# Create experiment for control tests\n",
    "control_exp_id = db.create_experiment(\n",
    "    name=\"Control Tests - Baseline\",\n",
    "    mitigation_strategy=\"baseline\",\n",
    "    description=\"Well-established facts that should NOT produce hallucinations.\"\n",
    ")\n",
    "\n",
    "print(f\"Unintentional tests experiment ID: {unintentional_exp_id}\")\n",
    "print(f\"Control tests experiment ID: {control_exp_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test vectors\n",
    "unintentional_vectors = HallucinationTestVectors.get_unintentional_vectors()\n",
    "control_vectors = HallucinationTestVectors.get_control_vectors()\n",
    "\n",
    "print(f\"Unintentional test vectors: {len(unintentional_vectors)}\")\n",
    "print(f\"Control test vectors: {len(control_vectors)}\")\n",
    "\n",
    "print(\"\\nUnintentional test categories:\")\n",
    "categories = {}\n",
    "for vec in unintentional_vectors:\n",
    "    cat = vec['category']\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "for cat, count in sorted(categories.items()):\n",
    "    print(f\"  - {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Unintentional Hallucination Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing edge cases and knowledge boundaries...\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(unintentional_vectors, desc=\"Unintentional tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query model\n",
    "    response, metadata = agent.query_baseline(prompt)\n",
    "    \n",
    "    # Display\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Test {i+1}/{len(unintentional_vectors)}\")\n",
    "    print(f\"Category: {vector['category']}\")\n",
    "    print(f\"Description: {vector['description']}\")\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"\\nResponse:\\n{response}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Manual annotation needed for these (expected_hallucination may be None)\n",
    "    # For automated run, default to False if None\n",
    "    is_hallucination = vector.get('expected_hallucination', False)\n",
    "    \n",
    "    # Uncomment for manual review:\n",
    "    # annotation = input(\"\\nDid the model hallucinate? (y/n/u for uncertain): \").strip().lower()\n",
    "    # is_hallucination = annotation == 'y'\n",
    "    \n",
    "    # Determine hallucination type\n",
    "    hallucination_type = vector['category'] if is_hallucination else 'none'\n",
    "    \n",
    "    # Log to database\n",
    "    db.log_test(\n",
    "        experiment_id=unintentional_exp_id,\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category=vector['category'],\n",
    "        vector_type='unintentional',\n",
    "        expected_hallucination=vector.get('expected_hallucination'),\n",
    "        hallucination_type=hallucination_type,\n",
    "        severity=vector.get('severity', 'low'),\n",
    "        description=vector['description'],\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "print(\"\\n✓ Unintentional hallucination tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Control Tests\n",
    "\n",
    "These are well-established facts. The model should NOT hallucinate on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing control questions (should NOT hallucinate)...\\n\")\n",
    "\n",
    "for i, vector in enumerate(tqdm(control_vectors, desc=\"Control tests\")):\n",
    "    prompt = vector['prompt']\n",
    "    \n",
    "    # Query model\n",
    "    response, metadata = agent.query_baseline(prompt)\n",
    "    \n",
    "    # Display\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Control Test {i+1}/{len(control_vectors)}\")\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"\\nResponse:\\n{response}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # These should be False (no hallucination expected)\n",
    "    is_hallucination = False\n",
    "    \n",
    "    # Uncomment to verify:\n",
    "    # check = input(\"\\nDid it hallucinate? (y/n): \").strip().lower()\n",
    "    # is_hallucination = check == 'y'\n",
    "    \n",
    "    # Log to database\n",
    "    db.log_test(\n",
    "        experiment_id=control_exp_id,\n",
    "        prompt_text=prompt,\n",
    "        response_text=response,\n",
    "        is_hallucination=is_hallucination,\n",
    "        prompt_category='control',\n",
    "        vector_type='control',\n",
    "        expected_hallucination=False,\n",
    "        hallucination_type='none',\n",
    "        severity='low',\n",
    "        description=vector['description'],\n",
    "        response_time_ms=metadata.get('response_time_ms', 0),\n",
    "        tokens_used=metadata.get('tokens_used', 0)\n",
    "    )\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n✓ Control tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unintentional results\n",
    "df_unintentional = db.get_experiment_results(unintentional_exp_id)\n",
    "\n",
    "print(\"Unintentional Hallucination Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total tests: {len(df_unintentional)}\")\n",
    "print(f\"Hallucinations: {df_unintentional['is_hallucination'].sum()}\")\n",
    "print(f\"Hallucination rate: {df_unintentional['is_hallucination'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\nBy category:\")\n",
    "category_stats = df_unintentional.groupby('prompt_category')['is_hallucination'].agg(['count', 'sum', 'mean'])\n",
    "category_stats.columns = ['Total', 'Hallucinations', 'Rate']\n",
    "print(category_stats.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control results\n",
    "df_control = db.get_experiment_results(control_exp_id)\n",
    "\n",
    "print(\"Control Test Results (should be 0% hallucination)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total tests: {len(df_control)}\")\n",
    "print(f\"Hallucinations: {df_control['is_hallucination'].sum()}\")\n",
    "print(f\"Hallucination rate: {df_control['is_hallucination'].mean()*100:.1f}%\")\n",
    "\n",
    "if df_control['is_hallucination'].sum() > 0:\n",
    "    print(\"\\n⚠️  WARNING: Model hallucinated on control questions!\")\n",
    "    print(\"These are well-known facts. Review the responses.\")\n",
    "else:\n",
    "    print(\"\\n✓ Good: No hallucinations on control questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export both experiments\n",
    "unintentional_path = db.export_to_csv(unintentional_exp_id)\n",
    "control_path = db.export_to_csv(control_exp_id)\n",
    "\n",
    "print(f\"Unintentional results: {unintentional_path}\")\n",
    "print(f\"Control results: {control_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "**Document your findings:**\n",
    "\n",
    "1. **Knowledge Boundaries:**\n",
    "   - How does the model handle questions beyond its knowledge cutoff?\n",
    "   - Does it admit uncertainty or fabricate?\n",
    "\n",
    "2. **Ambiguous Queries:**\n",
    "   - Does the model invent specifics for vague questions?\n",
    "   - Does it ask for clarification?\n",
    "\n",
    "3. **Statistical Claims:**\n",
    "   - Does it cite specific numbers without sources?\n",
    "   - How confident does it sound?\n",
    "\n",
    "4. **Control Performance:**\n",
    "   - Did any well-known facts get incorrect responses?\n",
    "\n",
    "**Your notes:**\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **03_comparative_analysis.ipynb** to test how mitigation strategies (RAG, Constitutional AI, Chain-of-Thought) perform on these same prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
